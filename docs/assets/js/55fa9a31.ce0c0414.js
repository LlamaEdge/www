"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[284],{3469:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>i,toc:()=>p});var o=n(4848),s=n(8453);const a={sidebar_position:2},r="Create a chatbot LLM app",i={id:"developer-guide/chatbot-llm-app",title:"Create a chatbot LLM app",description:'The most common LLM app has to be the chatbot. For that, the base LLM is finetuned with a lot of back and forth conversation examples. The base LLM "learns" how to follow conversations and becomes a chat LLM. Since the conversation examples are fed into the LLM using certain formats, the chat LLM will expect the input prompt to follow the same format. This is called the prompt template. Let\'s see how that works.',source:"@site/docs/developer-guide/chatbot-llm-app.md",sourceDirName:"developer-guide",slug:"/developer-guide/chatbot-llm-app",permalink:"/docs/docs/developer-guide/chatbot-llm-app",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/developer-guide/chatbot-llm-app.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Create a basic LLM app",permalink:"/docs/docs/developer-guide/basic-llm-app"},next:{title:"Create a multimodal app",permalink:"/docs/docs/developer-guide/multimodal-app"}},l={},p=[{value:"Build and run",id:"build-and-run",level:2},{value:"The prompt template",id:"the-prompt-template",level:2},{value:"Code walkthrough",id:"code-walkthrough",level:2},{value:"Streaming response",id:"streaming-response",level:2}];function c(e){const t={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.h1,{id:"create-a-chatbot-llm-app",children:"Create a chatbot LLM app"}),"\n",(0,o.jsx)(t.p,{children:'The most common LLM app has to be the chatbot. For that, the base LLM is finetuned with a lot of back and forth conversation examples. The base LLM "learns" how to follow conversations and becomes a chat LLM. Since the conversation examples are fed into the LLM using certain formats, the chat LLM will expect the input prompt to follow the same format. This is called the prompt template. Let\'s see how that works.'}),"\n",(0,o.jsx)(t.h2,{id:"build-and-run",children:"Build and run"}),"\n",(0,o.jsx)(t.p,{children:"First, let's get the source code."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"git clone https://github.com/second-state/WasmEdge-WASINN-examples\ncd WasmEdge-WASINN-examples\ncd wasmedge-ggml/llama\n"})}),"\n",(0,o.jsxs)(t.p,{children:["Next, build it using the Rust ",(0,o.jsx)(t.code,{children:"cargo"})," tool."]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"cargo build --target wasm32-wasi --release\ncp target/wasm32-wasi/release/wasmedge-ggml-llama.wasm .\n"})}),"\n",(0,o.jsx)(t.p,{children:"Download a chat LLM."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"curl -LO https://huggingface.co/second-state/Llama-2-7B-Chat-GGUF/resolve/main/Llama-2-7b-chat-hf-Q5_K_M.gguf\n"})}),"\n",(0,o.jsxs)(t.p,{children:["Run it! We load the LLM under the name ",(0,o.jsx)(t.code,{children:"default"})," and then ask the ",(0,o.jsx)(t.code,{children:"wasmedge-ggml-llama.wasm"})," app to run the ",(0,o.jsx)(t.code,{children:"default"})," model."]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"wasmedge --dir .:. \\\n  --nn-preload default:GGML:AUTO:Llama-2-7b-chat-hf-Q5_K_M.gguf \\\n  wasmedge-ggml-llama.wasm default\n"})}),"\n",(0,o.jsx)(t.p,{children:"You can now converse with it on the command line."}),"\n",(0,o.jsx)(t.h2,{id:"the-prompt-template",children:"The prompt template"}),"\n",(0,o.jsxs)(t.p,{children:['The prompt to the Llama2 LLM must follow the exact same template format it was finetuned on. It is as follows. As you can see, there is a "system prompt" and followed by back-and-forth conversations, ending with the user\'s new question or prompt. When the LLM answers, we can simply append the answer to the end of the prompt, and then put the next question in ',(0,o.jsx)(t.code,{children:"[INST]...[/INST]"}),"."]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"<s>[INST] <<SYS>>\nYou are a helpful assistant. Be polite!\n<</SYS>>\n\nMy first question? [/INST] The first answer. </s><s>[INST] My second question? [/INST] The second answer.</s><s>[INST] My third question? [/INST]\n"})}),"\n",(0,o.jsxs)(t.blockquote,{children:["\n",(0,o.jsxs)(t.p,{children:["Llama2 is just one of the prompt templates for chat. We also have examples for the ",(0,o.jsx)(t.a,{href:"https://github.com/second-state/WasmEdge-WASINN-examples/tree/master/wasmedge-ggml/chatml",children:"chatml template"})," and the ",(0,o.jsx)(t.a,{href:"https://github.com/second-state/WasmEdge-WASINN-examples/tree/master/wasmedge-ggml/gemma",children:"gemma template"}),"."]}),"\n"]}),"\n",(0,o.jsx)(t.h2,{id:"code-walkthrough",children:"Code walkthrough"}),"\n",(0,o.jsxs)(t.p,{children:["The source code of this project is ",(0,o.jsx)(t.a,{href:"https://github.com/second-state/WasmEdge-WASINN-examples/blob/master/wasmedge-ggml/llama/src/main.rs",children:"here"}),". It starts the execution context with ",(0,o.jsx)(t.code,{children:"options"})," and sends in the prompt to ",(0,o.jsx)(t.code,{children:"compute()"}),"."]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'let graph = GraphBuilder::new(GraphEncoding::Ggml, ExecutionTarget::AUTO)\n    .config(serde_json::to_string(&options).expect("Failed to serialize options"))\n    .build_from_cache(model_name)\n    .expect("Failed to build graph");\nlet mut context = graph\n    .init_execution_context()\n    .expect("Failed to init context");\n\n... ...\n\nlet tensor_data = prompt.as_bytes().to_vec();\ncontext.set_input(0, TensorType::U8, &[1], &tensor_data).expect("Failed to set input");\ncontext.compute().expect("Failed to compute");\nlet output = get_output_from_context(&context);\nprintln!("{}", output.trim());\n'})}),"\n",(0,o.jsx)(t.p,{children:"The interesting part, however, is how we construct the prompt. It starts with the system prompt."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'let mut saved_prompt = String::new();\nlet system_prompt = String::from("You are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe." );\n'})}),"\n",(0,o.jsx)(t.p,{children:"Then, in the question and answer loop, we will append the question, run the inference, and then append the answer to the prompt according to the template."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'loop {\n    let input = read_input();\n    if saved_prompt.is_empty() {\n        saved_prompt = format!(\n            "[INST] <<SYS>> {} <</SYS>> {} [/INST]",\n            system_prompt, input\n        );\n    } else {\n        saved_prompt = format!("{} [INST] {} [/INST]", saved_prompt, input);\n    }\n\n    ... ...\n\n    match context.compute() {\n        ... ....\n    }\n    let mut output = get_output_from_context(&context);\n    println!("ASSISTANT:\\n{}", output.trim());\n\n    // Update the saved prompt.\n    output = output.trim().to_string();\n    saved_prompt = format!("{} {}", saved_prompt, output);\n}\n'})}),"\n",(0,o.jsx)(t.h2,{id:"streaming-response",children:"Streaming response"}),"\n",(0,o.jsxs)(t.p,{children:["An important usability feature of chatbot apps is to stream LLM responses back to the user. LlamaEdge provides APIs that allow the application to retrieve the LLM responses one word at a time. We have a ",(0,o.jsx)(t.a,{href:"https://github.com/second-state/WasmEdge-WASINN-examples/blob/master/wasmedge-ggml/llama-stream/",children:"complete example here"}),". Instead of calling ",(0,o.jsx)(t.code,{children:"compute()"})," on the execution context, you should call ",(0,o.jsx)(t.code,{children:"compute_single()"})," instead. The following code retrieves the response one token at a time in a loop and prints the token as it arrives."]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'println!("ASSISTANT:");\nloop {\n    match context.compute_single() {\n        ... ...\n    }\n    // Retrieve the single output token and print it.\n    let token = get_single_output_from_context(&context);\n    print!("{}", token);\n    io::stdout().flush().unwrap();\n    }\n    println!();\n}\n'})}),"\n",(0,o.jsxs)(t.p,{children:["The ",(0,o.jsx)(t.code,{children:"get_single_output_from_context()"})," helper function calls the new API function ",(0,o.jsx)(t.code,{children:"get_output_single()"})," on the execution context."]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'fn get_single_output_from_context(context: &GraphExecutionContext) -> String {\n    get_data_from_context(context, 0, true)\n}\n\nfn get_data_from_context(context: &GraphExecutionContext, index: usize, is_single: bool) -> String {\n    // Preserve for 4096 tokens with average token length 6\n    const MAX_OUTPUT_BUFFER_SIZE: usize = 4096 * 6;\n    let mut output_buffer = vec![0u8; MAX_OUTPUT_BUFFER_SIZE];\n    let mut output_size = if is_single {\n        context\n            .get_output_single(index, &mut output_buffer)\n            .expect("Failed to get single output")\n    } else {\n        context\n            .get_output(index, &mut output_buffer)\n            .expect("Failed to get output")\n    };\n    output_size = std::cmp::min(MAX_OUTPUT_BUFFER_SIZE, output_size);\n\n    return String::from_utf8_lossy(&output_buffer[..output_size]).to_string();\n}\n'})}),"\n",(0,o.jsx)(t.p,{children:"That's it!"})]})}function d(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>i});var o=n(6540);const s={},a=o.createContext(s);function r(e){const t=o.useContext(a);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),o.createElement(a.Provider,{value:t},e.children)}}}]);