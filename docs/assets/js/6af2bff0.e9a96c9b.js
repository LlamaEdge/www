"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1258],{2064:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var d=t(4848),r=t(8453);const i={sidebar_position:4},o="Create an embedding app",s={id:"inference-sdk/embedding-app",title:"Create an embedding app",description:'An important LLM task is to generate embeddings for natural language sentences. It converts a sentence to a vector of numbers called an "embedding". The embedding vectors can then be stored in a vector database. You can search it later to find similiar sentences.',source:"@site/docs/inference-sdk/embedding-app.md",sourceDirName:"inference-sdk",slug:"/inference-sdk/embedding-app",permalink:"/docs/inference-sdk/embedding-app",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/inference-sdk/embedding-app.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Create a chatbot LLM app",permalink:"/docs/inference-sdk/chatbot-llm-app"}},a={},c=[{value:"Build and run",id:"build-and-run",level:2},{value:"Code walkthrough",id:"code-walkthrough",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,d.jsxs)(d.Fragment,{children:[(0,d.jsx)(n.h1,{id:"create-an-embedding-app",children:"Create an embedding app"}),"\n",(0,d.jsx)(n.p,{children:'An important LLM task is to generate embeddings for natural language sentences. It converts a sentence to a vector of numbers called an "embedding". The embedding vectors can then be stored in a vector database. You can search it later to find similiar sentences.'}),"\n",(0,d.jsx)(n.h2,{id:"build-and-run",children:"Build and run"}),"\n",(0,d.jsx)(n.p,{children:"First, let's get the source code."}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{children:"git clone https://github.com/second-state/WasmEdge-WASINN-examples\ncd WasmEdge-WASINN-examples\ncd wasmedge-ggml/embedding\n"})}),"\n",(0,d.jsxs)(n.p,{children:["Next, build it using the Rust ",(0,d.jsx)(n.code,{children:"cargo"})," tool."]}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{children:"cargo build --target wasm32-wasip1 --release\ncp target/wasm32-wasip1/release/wasmedge-ggml-llama-embedding.wasm .\n"})}),"\n",(0,d.jsx)(n.p,{children:"Download an embedding model."}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{children:"curl -LO https://huggingface.co/second-state/All-MiniLM-L6-v2-Embedding-GGUF/resolve/main/all-MiniLM-L6-v2-ggml-model-f16.gguf\n"})}),"\n",(0,d.jsxs)(n.p,{children:["Run it! We load the embedding model under the name ",(0,d.jsx)(n.code,{children:"default"})," and then ask the ",(0,d.jsx)(n.code,{children:"wasmedge-ggml-llama-embedding.wasm"})," app to run the ",(0,d.jsx)(n.code,{children:"default"})," model."]}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{children:"$ wasmedge --dir .:. \\\n  --nn-preload default:GGML:AUTO:all-MiniLM-L6-v2-ggml-model-f16.gguf \\\n  wasmedge-ggml-llama-embedding.wasm default\n"})}),"\n",(0,d.jsx)(n.p,{children:"Now, you can enter a prompt and the program will use the embedding model to generate the embedding vector for you!"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{children:'Prompt:\nWhat\'s the capital of the United States?\nRaw Embedding Output: {"n_embedding": 384, "embedding": [0.5426152349,-0.03840282559,-0.03644151986,0.3677068651,-0.115977712...(omitted)...,-0.003531290218]}\nInteract with Embedding:\nN_Embd: 384\nShow the first 5 elements:\nembd[0] = 0.5426152349\nembd[1] = -0.03840282559\nembd[2] = -0.03644151986\nembd[3] = 0.3677068651\nembd[4] = -0.115977712\n'})}),"\n",(0,d.jsx)(n.h2,{id:"code-walkthrough",children:"Code walkthrough"}),"\n",(0,d.jsxs)(n.p,{children:["The Rust source code for this project is ",(0,d.jsx)(n.a,{href:"https://github.com/second-state/WasmEdge-WASINN-examples/blob/master/wasmedge-ggml/embedding/src/main.rs",children:"here"}),". First, we start the execution context with the ",(0,d.jsx)(n.code,{children:"--nn-preload"})," model by its name."]}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{children:'let graph = GraphBuilder::new(GraphEncoding::Ggml, ExecutionTarget::AUTO)\n    .config(options.to_string())\n    .build_from_cache(model_name)\n    .expect("Create GraphBuilder Failed, please check the model name or options");\nlet mut context = graph\n    .init_execution_context()\n    .expect("Init Context Failed, please check the model");\n'})}),"\n",(0,d.jsxs)(n.p,{children:["Then we call the ",(0,d.jsx)(n.code,{children:"compute()"})," function on the execution context, and pass in a sentence to compute an embedding vector."]}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{children:"let tensor_data = prompt.as_bytes().to_vec();\ncontext.set_input(0, TensorType::U8, &[1], &tensor_data).unwrap();\ncontext.compute().unwrap();\n"})}),"\n",(0,d.jsxs)(n.p,{children:["You can then retrieve the generated embedding vector from the execution context. The embedding data is a JSON structure. The ",(0,d.jsx)(n.code,{children:"n_embedding"})," field is the size of embedding vector. This vector size is determined by the embedding model itself. That is, an embedding model will always generate embeddings of the same size. The ",(0,d.jsx)(n.code,{children:"embedding"})," field is the array for the vector data."]}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{children:'let embd = get_embd_from_context(&context);\nlet n_embd = embd["n_embedding"].as_u64().unwrap();\n\nprintln!("Show the first 5 elements:");\nfor idx in 0..5 {\n    println!("embd[{}] = {}", idx, embd["embedding"][idx as usize]);\n}\n'})}),"\n",(0,d.jsxs)(n.p,{children:["The ",(0,d.jsx)(n.code,{children:"get_embd_from_context()"})," function is straightforward. It simply retrieves data from the execution context's output buffer."]}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{children:"fn get_embd_from_context(context: &GraphExecutionContext) -> Value {\n    serde_json::from_str(&get_data_from_context(context, 0)).unwrap()\n}\n\nfn get_data_from_context(context: &GraphExecutionContext, index: usize) -> String {\n    // Preserve for 4096 tokens with average token length 15\n    const MAX_OUTPUT_BUFFER_SIZE: usize = 4096 * 15 + 128;\n    let mut output_buffer = vec![0u8; MAX_OUTPUT_BUFFER_SIZE];\n    let mut output_size = context.get_output(index, &mut output_buffer).unwrap();\n    output_size = std::cmp::min(MAX_OUTPUT_BUFFER_SIZE, output_size);\n\n    String::from_utf8_lossy(&output_buffer[..output_size]).to_string()\n}\n"})}),"\n",(0,d.jsxs)(n.p,{children:["You can upsert the ",(0,d.jsx)(n.code,{children:'embd["embedding"]'})," data structure to any vector database you might use."]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,d.jsx)(n,{...e,children:(0,d.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>s});var d=t(6540);const r={},i=d.createContext(r);function o(e){const n=d.useContext(i);return d.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),d.createElement(i.Provider,{value:n},e.children)}}}]);