"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[76],{2755:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>l,toc:()=>r});var n=a(4848),i=a(8453);const s={sidebar_position:2},o="Getting started with LlamaEdge",l={id:"user-guide/get-started-with-llamaedge",title:"Getting started with LlamaEdge",description:"Let's dive into a simple and practical tutorial on getting started with LlamaEdge, focusing on how to use a Command Line Interface (CLI) installer to run a model, along with some useful WasmEdge commands. This guide can be adjusted and applied to run Llama 2 series of models, tailored to give you a hands-on approach to running your large language model with LlamaEdge.",source:"@site/docs/user-guide/get-started-with-llamaedge.md",sourceDirName:"user-guide",slug:"/user-guide/get-started-with-llamaedge",permalink:"/docs/docs/user-guide/get-started-with-llamaedge",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/user-guide/get-started-with-llamaedge.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Quickstart with one line of command",permalink:"/docs/docs/user-guide/quick-start-command"},next:{title:"Server-side RAG with LlamaEdge",permalink:"/docs/docs/user-guide/server-side-rag"}},d={},r=[{value:"Step 1: Installing WasmEdge",id:"step-1-installing-wasmedge",level:3},{value:"Step 2: Downloading the Model",id:"step-2-downloading-the-model",level:3},{value:"Step 3: Downloading the Wasm Application",id:"step-3-downloading-the-wasm-application",level:3},{value:"Step 4: Running the Model",id:"step-4-running-the-model",level:3},{value:"Optional: Creating an OpenAI-Compatible API Service",id:"optional-creating-an-openai-compatible-api-service",level:3}];function c(e){const t={code:"code",h1:"h1",h3:"h3",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"getting-started-with-llamaedge",children:"Getting started with LlamaEdge"}),"\n",(0,n.jsx)(t.p,{children:"Let's dive into a simple and practical tutorial on getting started with LlamaEdge, focusing on how to use a Command Line Interface (CLI) installer to run a model, along with some useful WasmEdge commands. This guide can be adjusted and applied to run Llama 2 series of models, tailored to give you a hands-on approach to running your large language model with LlamaEdge."}),"\n",(0,n.jsx)(t.h3,{id:"step-1-installing-wasmedge",children:"Step 1: Installing WasmEdge"}),"\n",(0,n.jsx)(t.p,{children:"First off, you'll need WasmEdge, a high-performance, lightweight, and extensible WebAssembly (Wasm) runtime optimized for server-side and edge computing. To install WasmEdge along with the necessary plugin for AI inference, open your terminal and execute the following command:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install.sh | bash -s -- --plugin wasi_nn-ggml\n"})}),"\n",(0,n.jsx)(t.p,{children:"This command fetches and runs the WasmEdge installation script, which automatically installs WasmEdge and the WASI-NN plugin, essential for running LLM models like Llama 2."}),"\n",(0,n.jsx)(t.h3,{id:"step-2-downloading-the-model",children:"Step 2: Downloading the Model"}),"\n",(0,n.jsxs)(t.p,{children:["Next, you'll need to obtain a model file. For this tutorial, we're focusing on the ",(0,n.jsx)(t.strong,{children:"Llama 2-13B model"}),", but the steps are generally applicable to other models too. Use the following command to download the model file:"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"curl -LO https://huggingface.co/second-state/Llama-2-13B-Chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf\n"})}),"\n",(0,n.jsx)(t.p,{children:"This command downloads the Llama 2-13B model from Hugging Face, a platform hosting various LLM models."}),"\n",(0,n.jsx)(t.h3,{id:"step-3-downloading-the-wasm-application",children:"Step 3: Downloading the Wasm Application"}),"\n",(0,n.jsx)(t.p,{children:"To interact with the model, you'll need a Wasm application. For a chat application, use:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"curl -LO https://github.com/second-state/LlamaEdge/releases/latest/download/llama-chat.wasm\n"})}),"\n",(0,n.jsx)(t.p,{children:"This downloads a portable Wasm file that lets you chat with the Llama 2 model directly from the command line."}),"\n",(0,n.jsx)(t.h3,{id:"step-4-running-the-model",children:"Step 4: Running the Model"}),"\n",(0,n.jsx)(t.p,{children:"With everything set up, it's time to run the model:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"wasmedge --dir .:. --nn-preload default:GGML:AUTO:llama-2-13b-chat.Q5_K_M.gguf llama-chat.wasm -p llama-2-chat\n"})}),"\n",(0,n.jsxs)(t.p,{children:["This command executes the chat application, allowing you to start interacting with the Llama 2 13b model. Here, ",(0,n.jsx)(t.code,{children:"wasmedge"})," is the command to run the WasmEdge runtime, ",(0,n.jsx)(t.code,{children:"--nn-preload"})," specifies the model to use with the WASI-NN plugin, and ",(0,n.jsx)(t.code,{children:"-p"})," sets the prompt template for the chat."]}),"\n",(0,n.jsx)(t.h3,{id:"optional-creating-an-openai-compatible-api-service",children:"Optional: Creating an OpenAI-Compatible API Service"}),"\n",(0,n.jsx)(t.p,{children:"If you're interested in creating a web API service that's compatible with OpenAI, download the API server application:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"curl -LO https://github.com/second-state/LlamaEdge/releases/latest/download/llama-api-server.wasm\n"})}),"\n",(0,n.jsx)(t.p,{children:"Then, start the API server:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"wasmedge --dir .:. --nn-preload default:GGML:AUTO:llama-2-13b-chat.Q5_K_M.gguf llama-api-server.wasm -p llama-2-chat\n"})}),"\n",(0,n.jsxs)(t.p,{children:["You can interact with this API using tools like ",(0,n.jsx)(t.code,{children:"curl"})," from another terminal:"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:'curl -X POST http://0.0.0.0:8080/v1/chat/completions -H \'accept:application/json\' -H \'Content-Type: application/json\' -d \'{"messages":[{"role":"system", "content":"You are a helpful AI assistant"}, {"role":"user", "content":"What is the capital of France?"}], "model":"llama-2-13b-chat"}\'\n'})}),"\n",(0,n.jsx)(t.p,{children:"That's it! You've just learned how to set up and run a Llama model using LlamaEdge. These steps demonstrate the ease and flexibility of deploying AI models across various devices without worrying about complex dependencies."})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},8453:(e,t,a)=>{a.d(t,{R:()=>o,x:()=>l});var n=a(6540);const i={},s=n.createContext(i);function o(e){const t=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),n.createElement(s.Provider,{value:t},e.children)}}}]);