"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[162],{4114:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>d});var n=t(4848),o=t(8453);const s={sidebar_position:1},l="Quick start with the Llava models",i={id:"user-guide/multimodal/llava",title:"Quick start with the Llava models",description:"Llava-v1.6-Vicuna-7B is open-source community's answer to OpenAI's multimodal GPT-4-V. It is also known as a Visual Language Model for its ability to handle visual images and language in a conversation.  This guide shows you how to set up and run Llava-v1.6-Vicuna-7B using the LlamaEdge Llama API server server, which provides an OpenAI-compatible API interface.",source:"@site/docs/user-guide/multimodal/llava.md",sourceDirName:"user-guide/multimodal",slug:"/user-guide/multimodal/llava",permalink:"/docs/user-guide/multimodal/llava",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/user-guide/multimodal/llava.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Multimodal",permalink:"/docs/category/multimodal"},next:{title:"Ecosystem apps",permalink:"/docs/category/ecosystem-apps"}},r={},d=[{value:"Step 1: Install WasmEdge",id:"step-1-install-wasmedge",level:3},{value:"Step 2: Download the LLM model",id:"step-2-download-the-llm-model",level:3},{value:"Step 3: Download a portable chatbot app",id:"step-3-download-a-portable-chatbot-app",level:3},{value:"Step 4: Chat with the chatbot UI",id:"step-4-chat-with-the-chatbot-ui",level:3}];function c(e){const a={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h3:"h3",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h1,{id:"quick-start-with-the-llava-models",children:"Quick start with the Llava models"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://huggingface.co/liuhaotian/llava-v1.6-vicuna-7b",children:"Llava-v1.6-Vicuna-7B"})," is open-source community's answer to OpenAI's multimodal GPT-4-V. It is also known as a Visual Language Model for its ability to handle visual images and language in a conversation.  This guide shows you how to set up and run Llava-v1.6-Vicuna-7B using the LlamaEdge Llama API server server, which provides an OpenAI-compatible API interface."]}),"\n",(0,n.jsx)(a.h3,{id:"step-1-install-wasmedge",children:"Step 1: Install WasmEdge"}),"\n",(0,n.jsx)(a.p,{children:"First off, you'll need WasmEdge, a high-performance, lightweight, and extensible WebAssembly (Wasm) runtime optimized for server-side and edge computing. To install WasmEdge along with the necessary plugin for AI inference, open your terminal and execute the following command:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s\n"})}),"\n",(0,n.jsx)(a.h3,{id:"step-2-download-the-llm-model",children:"Step 2: Download the LLM model"}),"\n",(0,n.jsxs)(a.p,{children:["Next, you'll need to obtain model files: the ",(0,n.jsx)(a.strong,{children:"Llava-v1.6-Vicuna-7B model"})," and the ",(0,n.jsx)(a.strong,{children:"mmproj model"}),"."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"curl -LO https://huggingface.co/second-state/Llava-v1.6-Vicuna-7B-GGUF/resolve/main/llava-v1.6-vicuna-7b-Q5_K_M.gguf\ncurl -LO https://huggingface.co/second-state/Llava-v1.6-Vicuna-7B-GGUF/resolve/main/llava-v1.6-vicuna-7b-mmproj-model-f16.gguf\n"})}),"\n",(0,n.jsx)(a.h3,{id:"step-3-download-a-portable-chatbot-app",children:"Step 3: Download a portable chatbot app"}),"\n",(0,n.jsxs)(a.p,{children:["Next, you need an application that can load the model and provide a UI to interact with the model.\nThe ",(0,n.jsx)(a.a,{href:"https://github.com/LlamaEdge/LlamaEdge/tree/main/llama-api-server",children:"LlamaEdge api server app"})," is a lightweight and cross-platform Wasm app that works on any device\nyou might have. Just download the compiled binary app."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"curl -LO https://github.com/second-state/LlamaEdge/releases/latest/download/llama-api-server.wasm\n"})}),"\n",(0,n.jsxs)(a.blockquote,{children:["\n",(0,n.jsx)(a.p,{children:"The LlamaEdge apps are written in Rust and compiled to portable Wasm. That means they can run across devices and OSes without any change to the binary apps. You can simply download and run the compiled wasm apps regardless of your platform."}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"step-4-chat-with-the-chatbot-ui",children:"Step 4: Chat with the chatbot UI"}),"\n",(0,n.jsxs)(a.p,{children:["The ",(0,n.jsx)(a.code,{children:"llama-api-server.wasm"})," is a web server with an OpenAI-compatible API. You still need HTML files for the chatbot UI.\nDownload and unzip the HTML UI files as follows."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"curl -LO https://github.com/LlamaEdge/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz\ntar xzf chatbot-ui.tar.gz\nrm chatbot-ui.tar.gz\n"})}),"\n",(0,n.jsx)(a.p,{children:"Then, start the web server."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"wasmedge --dir .:. --nn-preload default:GGML:AUTO:llava-v1.6-vicuna-7b-Q5_K_M.gguf llama-api-server.wasm -p vicuna-llava -c 4096 --llava-mmproj llava-v1.6-vicuna-7b-mmproj-model-f16.gguf -m llava-v1.6-vicuna-7b\n"})}),"\n",(0,n.jsxs)(a.p,{children:["Go to ",(0,n.jsx)(a.code,{children:"http://localhost:8080"})," on your computer to access the chatbot UI on a web page! You can upload an imange and chat with the model based on the image."]}),"\n",(0,n.jsx)(a.p,{children:"Congratulations! You have now started an multimodal app on your own device."})]})}function h(e={}){const{wrapper:a}={...(0,o.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},8453:(e,a,t)=>{t.d(a,{R:()=>l,x:()=>i});var n=t(6540);const o={},s=n.createContext(o);function l(e){const a=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function i(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:l(e.components),n.createElement(s.Provider,{value:a},e.children)}}}]);