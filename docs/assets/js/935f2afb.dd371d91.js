"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8581],{5610:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"LlamaEdge","href":"/docs/intro","docId":"intro","unlisted":false},{"type":"link","label":"LlamaEdge vs Python","href":"/docs/llamaedge_vs_python","docId":"llamaedge_vs_python","unlisted":false},{"type":"link","label":"LlamaEdge vs llama.cpp","href":"/docs/llamaedge_vs_llamacpp","docId":"llamaedge_vs_llamacpp","unlisted":false},{"type":"link","label":"LlamaEdge vs Ollama","href":"/docs/llamaedge_vs_ollama","docId":"llamaedge_vs_ollama","unlisted":false},{"type":"category","label":"Serve AI models","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Introduction","href":"/docs/ai-models/","docId":"ai-models/index","unlisted":false},{"type":"category","label":"LLM","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Quick start with LLM models","href":"/docs/ai-models/llm/quick-start-llm","docId":"ai-models/llm/quick-start-llm","unlisted":false},{"type":"link","label":"Calling external tools","href":"/docs/ai-models/llm/tool-call","docId":"ai-models/llm/tool-call","unlisted":false},{"type":"link","label":"API Reference","href":"/docs/ai-models/llm/api-reference","docId":"ai-models/llm/api-reference","unlisted":false}],"href":"/docs/category/llm"},{"type":"category","label":"Multimodal","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Quick start with the Llava models","href":"/docs/ai-models/multimodal/llava","docId":"ai-models/multimodal/llava","unlisted":false},{"type":"link","label":"Quick start with the Qwen 2.5 VL model","href":"/docs/ai-models/multimodal/qwen2-5","docId":"ai-models/multimodal/qwen2-5","unlisted":false},{"type":"link","label":"Quick start with the Gemma-3 model","href":"/docs/ai-models/multimodal/gemma-3","docId":"ai-models/multimodal/gemma-3","unlisted":false},{"type":"link","label":"Quick start with the MedGemma-4b model","href":"/docs/ai-models/multimodal/medgemma-4b","docId":"ai-models/multimodal/medgemma-4b","unlisted":false}],"href":"/docs/category/multimodal"},{"type":"category","label":"Embeddings","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Working with embedding models","href":"/docs/ai-models/embeddings/","docId":"ai-models/embeddings/index","unlisted":false}],"href":"/docs/category/embeddings"},{"type":"category","label":"Speech to Text","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Quick Start with Whisper","href":"/docs/ai-models/speech-to-text/quick-start-whisper","docId":"ai-models/speech-to-text/quick-start-whisper","unlisted":false},{"type":"link","label":"The API server CLI options","href":"/docs/ai-models/speech-to-text/cli","docId":"ai-models/speech-to-text/cli","unlisted":false},{"type":"link","label":"API Reference","href":"/docs/ai-models/speech-to-text/api-reference","docId":"ai-models/speech-to-text/api-reference","unlisted":false}],"href":"/docs/category/speech-to-text"},{"type":"category","label":"Text to Speech","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Run GPT-Sovits with LlamaEdge","href":"/docs/ai-models/text-to-speech/gpt-sovits","docId":"ai-models/text-to-speech/gpt-sovits","unlisted":false}],"href":"/docs/category/text-to-speech"},{"type":"category","label":"Text to Image","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Quick Start with Stable Diffusion models","href":"/docs/ai-models/text-to-image/quick-start-sd","docId":"ai-models/text-to-image/quick-start-sd","unlisted":false},{"type":"link","label":"Run FLUX models with LlamaEdge","href":"/docs/ai-models/text-to-image/flux","docId":"ai-models/text-to-image/flux","unlisted":false}],"href":"/docs/category/text-to-image"},{"type":"link","label":"Use LlamaEdge in Docker","href":"/docs/ai-models/llamaedge-docker","docId":"ai-models/llamaedge-docker","unlisted":false},{"type":"link","label":"Use LlamaEdge in Kubernetes","href":"/docs/ai-models/llamaedge-kubernetes","docId":"ai-models/llamaedge-kubernetes","unlisted":false}],"href":"/docs/category/serve-ai-models"},{"type":"category","label":"Agents and apps","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"What is Llama-Nexus","href":"/docs/llama-nexus/","docId":"llama-nexus/llama-nexus","unlisted":false},{"type":"link","label":"Manage Your API Services with Llama-Nexus","href":"/docs/llama-nexus/quick-start","docId":"llama-nexus/quick-start","unlisted":false},{"type":"link","label":"Register API services","href":"/docs/llama-nexus/register","docId":"llama-nexus/register","unlisted":false},{"type":"link","label":"CLI Options","href":"/docs/llama-nexus/cli","docId":"llama-nexus/cli","unlisted":false},{"type":"category","label":"Working with MCP servers","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Quick start with MCP servers","href":"/docs/llama-nexus/mcp/quick-start-with-mcp","docId":"llama-nexus/mcp/quick-start-with-mcp","unlisted":false},{"type":"link","label":"Agentic Search","href":"/docs/llama-nexus/mcp/agentic-search","docId":"llama-nexus/mcp/agentic-search","unlisted":false},{"type":"link","label":"Connect Another MCP Server","href":"/docs/llama-nexus/mcp/add-mcp-server","docId":"llama-nexus/mcp/add-mcp-server","unlisted":false}],"href":"/docs/category/working-with-mcp-servers"},{"type":"category","label":"Ecosystem apps","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Start the llama-nexus API service","href":"/docs/llama-nexus/openai-api/intro","docId":"llama-nexus/openai-api/intro","unlisted":false},{"type":"link","label":"LobeChat","href":"/docs/llama-nexus/openai-api/lobechat","docId":"llama-nexus/openai-api/lobechat","unlisted":false},{"type":"link","label":"AI coding assistant: Continue","href":"/docs/llama-nexus/openai-api/continue","docId":"llama-nexus/openai-api/continue","unlisted":false},{"type":"link","label":"Obsidian","href":"/docs/llama-nexus/openai-api/obsidian","docId":"llama-nexus/openai-api/obsidian","unlisted":false},{"type":"link","label":"FlowiseAI tool call","href":"/docs/llama-nexus/openai-api/flowiseai-tool-call","docId":"llama-nexus/openai-api/flowiseai-tool-call","unlisted":false},{"type":"link","label":"Translation Agent","href":"/docs/llama-nexus/openai-api/translation-agent","docId":"llama-nexus/openai-api/translation-agent","unlisted":false},{"type":"link","label":"LangChain","href":"/docs/llama-nexus/openai-api/langchain","docId":"llama-nexus/openai-api/langchain","unlisted":false},{"type":"link","label":"Agent Zero","href":"/docs/llama-nexus/openai-api/agent-zero","docId":"llama-nexus/openai-api/agent-zero","unlisted":false}],"href":"/docs/category/ecosystem-apps"}],"href":"/docs/category/agents-and-apps"},{"type":"category","label":"Embed AI models","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Create a basic LLM app","href":"/docs/inference-sdk/basic-llm-app","docId":"inference-sdk/basic-llm-app","unlisted":false},{"type":"link","label":"Create a chatbot LLM app","href":"/docs/inference-sdk/chatbot-llm-app","docId":"inference-sdk/chatbot-llm-app","unlisted":false},{"type":"link","label":"Create an embedding app","href":"/docs/inference-sdk/embedding-app","docId":"inference-sdk/embedding-app","unlisted":false}],"href":"/docs/category/embed-ai-models"}]},"docs":{"ai-models/embeddings/index":{"id":"ai-models/embeddings/index","title":"Working with embedding models","description":"Embedding models compute vectors from text inputs. The vectors can then be used as search index","sidebar":"tutorialSidebar"},"ai-models/index":{"id":"ai-models/index","title":"Introduction","description":"LlamaEdge is a versatile platform supporting multiple types of AI models. The most common use of LlamaEdge is to","sidebar":"tutorialSidebar"},"ai-models/llamaedge-docker":{"id":"ai-models/llamaedge-docker","title":"Use LlamaEdge in Docker","description":"You can run all the commands in this document without any change on any machine with the latest Docker and at least 8GB of RAM available to the container.","sidebar":"tutorialSidebar"},"ai-models/llamaedge-kubernetes":{"id":"ai-models/llamaedge-kubernetes","title":"Use LlamaEdge in Kubernetes","description":"Coming soon.","sidebar":"tutorialSidebar"},"ai-models/llm/api-reference":{"id":"ai-models/llm/api-reference","title":"API Reference","description":"Introduction","sidebar":"tutorialSidebar"},"ai-models/llm/quick-start-llm":{"id":"ai-models/llm/quick-start-llm","title":"Quick start with LLM models","description":"LlamaEdge is a suite of component libraries and command line tools for developers to embed and run LLMs in their own apps. The best way to quickly experience LlamaEdge is to use easy-to-use utilities built on top of it.","sidebar":"tutorialSidebar"},"ai-models/llm/tool-call":{"id":"ai-models/llm/tool-call","title":"Calling external tools","description":"Tool calling is one of the truly \\"LLM native\\" interaction modes that has never existed before.","sidebar":"tutorialSidebar"},"ai-models/multimodal/gemma-3":{"id":"ai-models/multimodal/gemma-3","title":"Quick start with the Gemma-3 model","description":"Gemma 3 introduces powerful vision-language capabilities across its 4B, 12B, and 27B models through a custom SigLIP vision encoder, enabling rich interpretation of visual input. It processes fixed-size 896x896 images using a \u201cPan&Scan\u201d algorithm for adaptive cropping and resizing, balancing detail preservation with computational cost","sidebar":"tutorialSidebar"},"ai-models/multimodal/llava":{"id":"ai-models/multimodal/llava","title":"Quick start with the Llava models","description":"Llava-v1.6-Vicuna-7B is open-source community\'s answer to OpenAI\'s multimodal GPT-4-V. It is also known as a Visual Language Model for its ability to handle visual images and language in a conversation.  This guide shows you how to set up and run Llava-v1.6-Vicuna-7B using the LlamaEdge Llama API server server, which provides an OpenAI-compatible API interface.","sidebar":"tutorialSidebar"},"ai-models/multimodal/medgemma-4b":{"id":"ai-models/multimodal/medgemma-4b","title":"Quick start with the MedGemma-4b model","description":"MedGemma-4b  is a collection of Gemma 3 variants that are trained for performance on medical text and image comprehension. Developers can use MedGemma to accelerate building healthcare-based AI applications. MedGemma currently comes in two variants: a 4B multimodal version and a 27B text-only version.","sidebar":"tutorialSidebar"},"ai-models/multimodal/qwen2-5":{"id":"ai-models/multimodal/qwen2-5","title":"Quick start with the Qwen 2.5 VL model","description":"Qwen 2.5 VL is the latest vision-language model from the Qwen series, designed to handle a wide range of complex multimodal tasks. It excels at understanding visual content such as text, charts, and layouts, and can act as an intelligent agent capable of interacting with tools and devices.","sidebar":"tutorialSidebar"},"ai-models/speech-to-text/api-reference":{"id":"ai-models/speech-to-text/api-reference","title":"API Reference","description":"","sidebar":"tutorialSidebar"},"ai-models/speech-to-text/cli":{"id":"ai-models/speech-to-text/cli","title":"The API server CLI options","description":"","sidebar":"tutorialSidebar"},"ai-models/speech-to-text/quick-start-whisper":{"id":"ai-models/speech-to-text/quick-start-whisper","title":"Quick Start with Whisper","description":"Whisper is OpenAI\'s general-purpose speech recognition model that accurately converts speech to text. This guide shows you how to set up and run Whisper using the LlamaEdge whisper API server server, which provides an OpenAI-compatible API interface.","sidebar":"tutorialSidebar"},"ai-models/text-to-image/flux":{"id":"ai-models/text-to-image/flux","title":"Run FLUX models with LlamaEdge","description":"FLUX.1 is an open-source image generation model developed by Black Forest Labs, the creators of Stable Diffusion. With the LlamaEdge stable-diffusion-api-server, you can build an OpenAI-compatible API server for FLUX models.","sidebar":"tutorialSidebar"},"ai-models/text-to-image/quick-start-sd":{"id":"ai-models/text-to-image/quick-start-sd","title":"Quick Start with Stable Diffusion models","description":"Stable Diffusion is a state-of-the-art text-to-image generation model that creates high-quality images from text descriptions. With the LlamaEdge stable-diffusion-api-server, you can build an OpenAI-compatible API server for Stable Diffusion models.","sidebar":"tutorialSidebar"},"ai-models/text-to-speech/gpt-sovits":{"id":"ai-models/text-to-speech/gpt-sovits","title":"Run GPT-Sovits with LlamaEdge","description":"","sidebar":"tutorialSidebar"},"inference-sdk/basic-llm-app":{"id":"inference-sdk/basic-llm-app","title":"Create a basic LLM app","description":"At the most basic level, the LLM completes text. That is why the input text is called a \\"prompt\\". The base model simply comes up with the next words that are likely to follow the prompt. In this example, we will demonstrate this basic use case.","sidebar":"tutorialSidebar"},"inference-sdk/chatbot-llm-app":{"id":"inference-sdk/chatbot-llm-app","title":"Create a chatbot LLM app","description":"The most common LLM app has to be the chatbot. For that, the base LLM is finetuned with a lot of back and forth conversation examples. The base LLM \\"learns\\" how to follow conversations and becomes a chat LLM. Since the conversation examples are fed into the LLM using certain formats, the chat LLM will expect the input prompt to follow the same format. This is called the prompt template. Let\'s see how that works.","sidebar":"tutorialSidebar"},"inference-sdk/embedding-app":{"id":"inference-sdk/embedding-app","title":"Create an embedding app","description":"An important LLM task is to generate embeddings for natural language sentences. It converts a sentence to a vector of numbers called an \\"embedding\\". The embedding vectors can then be stored in a vector database. You can search it later to find similiar sentences.","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"LlamaEdge","description":"LlamaEdge is the easiest & fastest way to run customized and fine-tuned LLMs locally or on the edge.","sidebar":"tutorialSidebar"},"llama-nexus/cli":{"id":"llama-nexus/cli","title":"CLI Options","description":"After installing the Llama-Nexus software, you can use the llama-nexus CLI to configure the setting.","sidebar":"tutorialSidebar"},"llama-nexus/llama-nexus":{"id":"llama-nexus/llama-nexus","title":"What is Llama-Nexus","description":"Llama-Nexus is a gateway service designed to manage and orchestrate OpenAI-compatible API servers.","sidebar":"tutorialSidebar"},"llama-nexus/mcp/add-mcp-server":{"id":"llama-nexus/mcp/add-mcp-server","title":"Connect Another MCP Server","description":"As an MCP client, Llama-Nexus supports connecting to multiple MCP servers. This article provides an example of how to add an additional MCP server.","sidebar":"tutorialSidebar"},"llama-nexus/mcp/agentic-search":{"id":"llama-nexus/mcp/agentic-search","title":"Agentic Search","description":"With its support for MCP servers, we can support \\"agentic search\\" from llama nexus. That is to support transparent knowledge base search via the /chat/completions API. There is no need for the llama nexus client to handle any tool call or MCP. It works like this:","sidebar":"tutorialSidebar"},"llama-nexus/mcp/quick-start-with-mcp":{"id":"llama-nexus/mcp/quick-start-with-mcp","title":"Quick start with MCP servers","description":"One of the key features of Llama-Nexus is its built-in MCP Client, which allows you to use Llama-Nexus for MCP-related tasks just like Claude Desktop and Cursor.","sidebar":"tutorialSidebar"},"llama-nexus/openai-api/agent-zero":{"id":"llama-nexus/openai-api/agent-zero","title":"Agent Zero","description":"Agent Zero is a general purpose AI agent application. You can simply ask it to accomplish tasks on the command line.","sidebar":"tutorialSidebar"},"llama-nexus/openai-api/continue":{"id":"llama-nexus/openai-api/continue","title":"AI coding assistant: Continue","description":"Continue is the leading open-source AI code assistant.","sidebar":"tutorialSidebar"},"llama-nexus/openai-api/flowiseai-tool-call":{"id":"llama-nexus/openai-api/flowiseai-tool-call","title":"FlowiseAI tool call","description":"FlowiseAI is a low-code tool for developers to build customized LLM orchestration flows & AI agents.","sidebar":"tutorialSidebar"},"llama-nexus/openai-api/intro":{"id":"llama-nexus/openai-api/intro","title":"Start the llama-nexus API service","description":"Since LlamaEdge provides an OpenAI-compatible API service, it can be a drop-in replacement for OpenAI in almost all LLM applications and frameworks.","sidebar":"tutorialSidebar"},"llama-nexus/openai-api/langchain":{"id":"llama-nexus/openai-api/langchain","title":"LangChain","description":"In this tutorial, I will introduce you how to build a client-side RAG using Llama2-7b-chat model, based on LlamaEdge and Langchain.","sidebar":"tutorialSidebar"},"llama-nexus/openai-api/lobechat":{"id":"llama-nexus/openai-api/lobechat","title":"LobeChat","description":"The LobeChat framework is a modern chatbot framework that supports a wide range of","sidebar":"tutorialSidebar"},"llama-nexus/openai-api/obsidian":{"id":"llama-nexus/openai-api/obsidian","title":"Obsidian","description":"Obsidian is a note-taking application that enables users to create, link, and visualize ideas directly on their devices. With Obsidian, you can seamlessly sync notes across devices, publish your work, and collaborate with others. The app is highly customizable, allowing users to enhance functionality through a wide range of plugins and themes. Its unique features include a graph view to visualize connections between notes, making it ideal for managing complex information and fostering creativity. Obsidian also emphasizes data privacy by storing notes locally.","sidebar":"tutorialSidebar"},"llama-nexus/openai-api/translation-agent":{"id":"llama-nexus/openai-api/translation-agent","title":"Translation Agent","description":"This LLM Translation Agent originally built by Prof. Andrew Ng is designed to facilitate accurate and efficient translation across multiple languages. It employs open source LLMs (Large Language Models) to provide high-quality translations. You can use your own fine-tuned models or any LLMs on Hugging Face like Meta\'s Llama 3. This documentation shows how the Transgenic Agent utilizes the Gemma-2-9B model for translation.","sidebar":"tutorialSidebar"},"llama-nexus/quick-start":{"id":"llama-nexus/quick-start","title":"Manage Your API Services with Llama-Nexus","description":"This tutorial shows you how to use Llama-Nexus to manage multiple OpenAI-compatible API services.","sidebar":"tutorialSidebar"},"llama-nexus/register":{"id":"llama-nexus/register","title":"Register API services","description":"You can add almost any OpenAI-compatible API services to the Llama-Nexus gateway.","sidebar":"tutorialSidebar"},"llamaedge_vs_llamacpp":{"id":"llamaedge_vs_llamacpp","title":"LlamaEdge vs llama.cpp","description":"The llama.cpp project is one of the inference backends for LlamaEdge. LlamaEdge provides high level application","sidebar":"tutorialSidebar"},"llamaedge_vs_ollama":{"id":"llamaedge_vs_ollama","title":"LlamaEdge vs Ollama","description":"There are several popular tools to run \\"local LLMs\\". Ollama is one of the earlist and most popular. Why do people","sidebar":"tutorialSidebar"},"llamaedge_vs_python":{"id":"llamaedge_vs_python","title":"LlamaEdge vs Python","description":"Most AI models are trained and even finetuned in Python / PyTorch, but you should not deploy and run them in Python.","sidebar":"tutorialSidebar"}}}')}}]);