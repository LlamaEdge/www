"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[680],{9140:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>a,contentTitle:()=>r,default:()=>p,frontMatter:()=>c,metadata:()=>i,toc:()=>d});var o=n(4848),s=n(8453);const c={sidebar_position:1},r="Create a basic LLM app",i={id:"developer-guide/basic-llm-app",title:"Create a basic LLM app",description:'At the most basic level, the LLM completes text. That is why the input text is called a "prompt". The base model simply comes up with the next words that are likely to follow the prompt. In this example, we will demonstrate this basic use case.',source:"@site/docs/developer-guide/basic-llm-app.md",sourceDirName:"developer-guide",slug:"/developer-guide/basic-llm-app",permalink:"/docs/docs/developer-guide/basic-llm-app",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/developer-guide/basic-llm-app.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Developer Guide",permalink:"/docs/docs/category/developer-guide"},next:{title:"Create a chatbot LLM app",permalink:"/docs/docs/developer-guide/chatbot-llm-app"}},a={},d=[{value:"Build and run",id:"build-and-run",level:2},{value:"Source code walkthrough",id:"source-code-walkthrough",level:2}];function l(e){const t={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.h1,{id:"create-a-basic-llm-app",children:"Create a basic LLM app"}),"\n",(0,o.jsx)(t.p,{children:'At the most basic level, the LLM completes text. That is why the input text is called a "prompt". The base model simply comes up with the next words that are likely to follow the prompt. In this example, we will demonstrate this basic use case.'}),"\n",(0,o.jsx)(t.h2,{id:"build-and-run",children:"Build and run"}),"\n",(0,o.jsx)(t.p,{children:"First, let's get the source code."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"git clone https://github.com/second-state/WasmEdge-WASINN-examples\ncd WasmEdge-WASINN-examples\ncd wasmedge-ggml/basic\n"})}),"\n",(0,o.jsxs)(t.p,{children:["Next, build it using the Rust ",(0,o.jsx)(t.code,{children:"cargo"})," tool."]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"cargo build --target wasm32-wasi --release\ncp target/wasm32-wasi/release/wasmedge-ggml-basic.wasm .\n"})}),"\n",(0,o.jsx)(t.p,{children:"Download a non-chat LLM. This one a code completion model. You give it a request and it will respond with code that meets your request."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"curl -LO https://huggingface.co/second-state/StarCoder2-7B-GGUF/resolve/main/starcoder2-7b-Q5_K_M.gguf\n"})}),"\n",(0,o.jsxs)(t.p,{children:["Run it! We load the LLM under the name ",(0,o.jsx)(t.code,{children:"default"})," and then ask the ",(0,o.jsx)(t.code,{children:"wasmedge-ggml-basic.wasm"})," app to run the ",(0,o.jsx)(t.code,{children:"default"})," model."]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"wasmedge --dir .:. \\\n  --env n_predict=100 \\\n  --nn-preload default:GGML:AUTO:starcoder2-7b-Q5_K_M.gguf \\\n  wasmedge-ggml-basic.wasm default\n"})}),"\n",(0,o.jsx)(t.p,{children:"Try a few examples. All those examples are to prompt the LLM to write code and complete the requested tasks."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"USER:\ndef print_hello_world():\n\nUSER:\nfn is_prime(n: u64) -> bool {\n\nUSER:\nWrite a Rust function to check if an input number is prime:\n"})}),"\n",(0,o.jsx)(t.h2,{id:"source-code-walkthrough",children:"Source code walkthrough"}),"\n",(0,o.jsxs)(t.p,{children:["The Rust source code for this example is ",(0,o.jsx)(t.a,{href:"https://github.com/second-state/WasmEdge-WASINN-examples/blob/master/wasmedge-ggml/basic/src/main.rs",children:"here"}),". The first omportant step in ",(0,o.jsx)(t.code,{children:"main()"})," is to create an execution context. The ",(0,o.jsx)(t.code,{children:"config()"})," function takes an ",(0,o.jsx)(t.code,{children:"options"})," struct that provides inference options for the model, such as context length, temperature etc. You can check the ",(0,o.jsx)(t.code,{children:"get_options_from_env()"})," function in the source code to see how the ",(0,o.jsx)(t.code,{children:"options"})," is constructed."]}),"\n",(0,o.jsxs)(t.blockquote,{children:["\n",(0,o.jsxs)(t.p,{children:["The ",(0,o.jsx)(t.code,{children:"model_name"})," is ",(0,o.jsx)(t.code,{children:"default"}),", which correspond to the model name in ",(0,o.jsx)(t.code,{children:"--nn-preload"}),"."]}),"\n"]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'let graph = GraphBuilder::new(GraphEncoding::Ggml, ExecutionTarget::AUTO)\n    .config(serde_json::to_string(&options).expect("Failed to serialize options"))\n    .build_from_cache(model_name)\n    .expect("Failed to build graph");\nlet mut context = graph\n    .init_execution_context()\n    .expect("Failed to init context");\n'})}),"\n",(0,o.jsxs)(t.p,{children:["Next, we simply pass the request prompt text to the execution context as a byte array, and call ",(0,o.jsx)(t.code,{children:"compute()"}),"."]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'let tensor_data = prompt.as_bytes().to_vec();\ncontext.set_input(0, TensorType::U8, &[1], &tensor_data).expect("Failed to set input");\ncontext.compute().expect("Failed to compute");\n'})}),"\n",(0,o.jsx)(t.p,{children:"Finally, you simply get the computed output from the execution context, and print it as a string."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'let output = get_output_from_context(&context);\nprintln!("{}", output.trim());\n'})}),"\n",(0,o.jsxs)(t.p,{children:["The above helper function ",(0,o.jsx)(t.code,{children:"get_output_from_context()"})," uses a buffer to read data from the context."]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'fn get_data_from_context(context: &GraphExecutionContext, index: usize) -> String {\n    // Preserve for 4096 tokens with average token length 6\n    const MAX_OUTPUT_BUFFER_SIZE: usize = 4096 * 6;\n    let mut output_buffer = vec![0u8; MAX_OUTPUT_BUFFER_SIZE];\n    let mut output_size = context\n        .get_output(index, &mut output_buffer)\n        .expect("Failed to get output");\n    output_size = std::cmp::min(MAX_OUTPUT_BUFFER_SIZE, output_size);\n\n    return String::from_utf8_lossy(&output_buffer[..output_size]).to_string();\n}\n'})}),"\n",(0,o.jsx)(t.p,{children:"That's it!"})]})}function p(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>i});var o=n(6540);const s={},c=o.createContext(s);function r(e){const t=o.useContext(c);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),o.createElement(c.Provider,{value:t},e.children)}}}]);