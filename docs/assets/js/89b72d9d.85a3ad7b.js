"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[74],{9412:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>l,toc:()=>c});var a=t(4848),i=t(8453);const s={sidebar_position:7},o="LangChain",l={id:"user-guide/openai-api/langchain",title:"LangChain",description:"In this tutorial, I will introduce you how to build a client-side RAG using Llama2-7b-chat model, based on LlamaEdge and Langchain.",source:"@site/docs/user-guide/openai-api/langchain.md",sourceDirName:"user-guide/openai-api",slug:"/user-guide/openai-api/langchain",permalink:"/docs/user-guide/openai-api/langchain",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/user-guide/openai-api/langchain.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_position:7},sidebar:"tutorialSidebar",previous:{title:"Translation Agent",permalink:"/docs/user-guide/openai-api/translation-agent"},next:{title:"Long-term memory and knowledge",permalink:"/docs/category/long-term-memory-and-knowledge"}},r={},c=[{value:"Build the client app using Langchian with vector DB support",id:"build-the-client-app-using-langchian-with-vector-db-support",level:3},{value:"Build an OpenAI compatible API server for the open source LLM using LlamaEdge",id:"build-an-openai-compatible-api-server-for-the-open-source-llm-using-llamaedge",level:3},{value:"Connect your self-hosted LLMs with the chatbot web app",id:"connect-your-self-hosted-llms-with-the-chatbot-web-app",level:3},{value:"What\u2019s next?",id:"whats-next",level:3}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h3:"h3",img:"img",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"langchain",children:"LangChain"}),"\n",(0,a.jsx)(n.p,{children:"In this tutorial, I will introduce you how to build a client-side RAG using Llama2-7b-chat model, based on LlamaEdge and Langchain."}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:["LlamaEdge has ",(0,a.jsx)(n.a,{href:"https://twitter.com/realwasmedge/status/1742437253107130552",children:"recently became"})," an official inference backend for LangChain, allowing LangChain applications to run open source LLMs on heterogeneous GPU devices."]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"build-the-client-app-using-langchian-with-vector-db-support",children:"Build the client app using Langchian with vector DB support"}),"\n",(0,a.jsx)(n.p,{children:"First, let's build a chatbot web app using Langchain. This part will be built in Python. The app includes uploading file and attaches the Chroma DB and the gpt4all embedding algorithms."}),"\n",(0,a.jsxs)(n.p,{children:["To quick start, fork or clone ",(0,a.jsx)(n.a,{href:"https://github.com/second-state/wasm-llm",children:"the wasm-llm repo"})," and open the wasm-bot folder."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"git clone https://github.com/second-state/wasm-llm.git\ncd wasm-llm/wasm-rag-service\n"})}),"\n",(0,a.jsx)(n.p,{children:"Next, let\u2019s install the required python dependencies for this program. We will use conda to control the version and environment."}),"\n",(0,a.jsxs)(n.p,{children:["Follow the ",(0,a.jsx)(n.a,{href:"https://docs.conda.io/projects/miniconda/en/latest/#quick-command-line-install",children:"miniconda installation instruction"})," to install mini conda your own machine. After that, create a conda environment for the chatbot web app. Let\u2019s use chatbot as the name."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"conda create -n wasm-rag python=3.11\nconda activate wasm-rag\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Then, you may notice that your terminal has entered the ",(0,a.jsx)(n.code,{children:"chatbot"})," environment. Let\u2019s ",(0,a.jsx)(n.a,{href:"https://github.com/second-state/wasm-llm/blob/main/wasm-bot/requirements.txt",children:"install the dependencies for this chatbot app"}),". All the dependencies are included in the ",(0,a.jsx)(n.code,{children:"requirements.txt"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"pip install -r requirements.txt\n"})}),"\n",(0,a.jsx)(n.p,{children:"With all dependencies installed, then we can execute the chatbot app."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"streamlit run app.py\n"})}),"\n",(0,a.jsx)(n.p,{children:"If everything goes well, you will see the following messages on your terminal. In the meanwhile,  a web page will be opened in your browser."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"You can now view your Streamlit app in your browser.\nLocal URL: http://localhost:8501\nNetwork URL: http://192.168.0.103:8501\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:"https://github.com/LlamaEdge/docs/assets/45785633/af418d8e-9377-4613-b976-4ed3bec1836c",alt:"image"})}),"\n",(0,a.jsx)(n.p,{children:"Now, we have completed the first part \u2014 a RAG client app waiting for a LLM backend to answer user\u2019s question."}),"\n",(0,a.jsx)(n.h3,{id:"build-an-openai-compatible-api-server-for-the-open-source-llm-using-llamaedge",children:"Build an OpenAI compatible API server for the open source LLM using LlamaEdge"}),"\n",(0,a.jsx)(n.p,{children:"Let\u2019s build a API server for the open source LLM with WasmEdge."}),"\n",(0,a.jsx)(n.p,{children:"First, install WasmEdge runtime with one single command line."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install.sh | bash -s -- --plugin wasi_nn-ggml\n"})}),"\n",(0,a.jsx)(n.p,{children:"Second, download the model file in GGUF file. Here, I use llama2-7b as an example. We have tried several LLMs and concluded that Llama2-7B is the beat for RAG applications."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"curl -LO https://huggingface.co/second-state/Llama-2-7B-Chat-GGUF/resolve/main/Llama-2-7b-chat-hf-Q5_K_M.gguf\n"})}),"\n",(0,a.jsx)(n.p,{children:"Third, download an API server app. It is a cross-platform portable Wasm app that can run on many CPU and GPU devices."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"curl -LO https://github.com/second-state/LlamaEdge/releases/latest/download/llama-api-server.wasm\n"})}),"\n",(0,a.jsx)(n.p,{children:"Finally, use the following command lines to start an API server for the model.  If you have did the above steps, just run the follwing command line."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"wasmedge --dir .:. --nn-preload default:GGML:AUTO:Llama-2-7b-chat-hf-Q5_K_M.gguf llama-api-server.wasm -p llama-2-chat -c 4096\n"})}),"\n",(0,a.jsx)(n.p,{children:"If everything goes well, the following information will be printed on the terminal."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"[INFO] Socket address: 0.0.0.0:8080\n[INFO] Model name: default\n[INFO] Model alias: default\n[INFO] Prompt context size: 512\n[INFO] Number of tokens to predict: 1024\n[INFO] Number of layers to run on the GPU: 100\n[INFO] Batch size for prompt processing: 512\n[INFO] Temperature for sampling: 0.8\n[INFO] Penalize repeat sequence of tokens: 1.1\n[INFO] Prompt template: HumanAssistant\n[INFO] Log prompts: false\n[INFO] Log statistics: false\n[INFO] Log all information: false\n[INFO] Starting server ...\nggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\nggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\nggml_init_cublas: found 1 CUDA devices:\n  Device 0: Orin, compute capability 8.7, VMM: yes\n[INFO] Plugin version: b1953 (commit 6f9939d1)\n[INFO] Listening on http://0.0.0.0:8080\n"})}),"\n",(0,a.jsx)(n.p,{children:"Now the Llama2-7B-Chat model is hosted at the port of 8080."}),"\n",(0,a.jsx)(n.h3,{id:"connect-your-self-hosted-llms-with-the-chatbot-web-app",children:"Connect your self-hosted LLMs with the chatbot web app"}),"\n",(0,a.jsx)(n.p,{children:"Go back to the web page opened in the first step. Click Use cusom service on the bottom left of page and click the Connect button.\nThen you will see a section to upload your own data locally.  Upload a pdf file here. When the uploading process is done, the bot will send you a message: \u201cHello \ud83d\udc4b, how can I help you?\u201d,  which is a ready sign."}),"\n",(0,a.jsx)(n.p,{children:"Ask a question, and the bot will reply to you based on the file you uploaded."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:"https://github.com/LlamaEdge/docs/assets/45785633/0b5273f6-7edd-4fcf-b917-c5931609c5db",alt:"image"})}),"\n",(0,a.jsx)(n.h3,{id:"whats-next",children:"What\u2019s next?"}),"\n",(0,a.jsx)(n.p,{children:"We will introduce you how to build such a client-side RAG app with OpenWebui"})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>l});var a=t(6540);const i={},s=a.createContext(i);function o(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);