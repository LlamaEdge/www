"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2431],{8035:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var r=t(4848),i=t(8453);const s={sidebar_position:1},a="Start an LlamaEdge API service",o={id:"user-guide/openai-api/intro",title:"Start an LlamaEdge API service",description:"Since LlamaEdge provides an OpenAI-compatible API service, it can be a drop-in replacement for OpenAI in almost all LLM applications and frameworks.",source:"@site/docs/user-guide/openai-api/intro.md",sourceDirName:"user-guide/openai-api",slug:"/user-guide/openai-api/intro",permalink:"/docs/user-guide/openai-api/intro",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/user-guide/openai-api/intro.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Ecosystem apps",permalink:"/docs/category/ecosystem-apps"},next:{title:"LobeChat",permalink:"/docs/user-guide/openai-api/lobechat"}},l={},c=[{value:"Start the API servers for multiple models",id:"start-the-api-servers-for-multiple-models",level:2},{value:"OpenAI replacement",id:"openai-replacement",level:2},{value:"The OpenAI Python library",id:"the-openai-python-library",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"start-an-llamaedge-api-service",children:"Start an LlamaEdge API service"}),"\n",(0,r.jsx)(n.p,{children:"Since LlamaEdge provides an OpenAI-compatible API service, it can be a drop-in replacement for OpenAI in almost all LLM applications and frameworks.\nCheckout the articles in this section for instructions and examples for how to use locally hosted LlamaEdge API services in popular LLM apps."}),"\n",(0,r.jsx)(n.h2,{id:"start-the-api-servers-for-multiple-models",children:"Start the API servers for multiple models"}),"\n",(0,r.jsx)(n.p,{children:"First, you will need to start an OpenAI compatible API server."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Start an OpenAI compatible API server for Large Language Models (LLM)\n\u2794 ",(0,r.jsx)(n.a,{href:"/docs/category/llm",children:"Get Started with LLM"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Start an OpenAI compatible API server for Whisper\n\u2794 ",(0,r.jsx)(n.a,{href:"/docs/category/speech-to-text",children:"Get Started with Speech to Text"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Start an OpenAI compatible API server for GPT-SOVITs and Piper\n\u2794 ",(0,r.jsx)(n.a,{href:"/docs/category/text-to-speech",children:"Get Started with Text to Speech"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Start an OpenAI compatible API server for Stable Diffusion and FLUX\n\u2794 ",(0,r.jsx)(n.a,{href:"/docs/category/text-to-image",children:"Get Started with Text-to-Image"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Start an OpenAI compatible API server for Llava and Qwen-VL\n\u2794 ",(0,r.jsx)(n.a,{href:"/docs/category/multimodal",children:"Get Started with Multimodal"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"openai-replacement",children:"OpenAI replacement"}),"\n",(0,r.jsx)(n.p,{children:"Now, you can ready to use this API server in OpenAI ecosystem apps as a drop-in replacement for the OpenAI API!\nIn general, for any OpenAI tool, you could just replace the following."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Config option"}),(0,r.jsx)(n.th,{children:"Value"}),(0,r.jsx)(n.th,{children:"Note"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"API endpoint URL"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"http://localhost:8080/v1"})}),(0,r.jsx)(n.td,{children:"If the server is accessible from the web, you could use the public IP and port"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Model Name (for LLM)"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"llama-3-8b-chat"})}),(0,r.jsxs)(n.td,{children:["The first value specified in the ",(0,r.jsx)(n.code,{children:"--model-name"})," option"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Model Name (for Text embedding)"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"nomic-embed"})}),(0,r.jsxs)(n.td,{children:["The second value specified in the ",(0,r.jsx)(n.code,{children:"--model-name"})," option"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"API key"}),(0,r.jsx)(n.td,{children:"Empty"}),(0,r.jsx)(n.td,{children:"Or any value if the app does not permit empty string"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"the-openai-python-library",children:"The OpenAI Python library"}),"\n",(0,r.jsxs)(n.p,{children:["You can install the ",(0,r.jsx)(n.a,{href:"https://pypi.org/project/openai/",children:"official OpenAI Python library"})," as follows."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"pip install openai\n"})}),"\n",(0,r.jsxs)(n.p,{children:["When you create an OpenAI client using the library, you can pass in the API endpoint point as the ",(0,r.jsx)(n.code,{children:"base_url"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'import openai\n\nclient = openai.OpenAI(base_url="http://localhost:8080/v1", api_key="")\n'})}),"\n",(0,r.jsx)(n.p,{children:"Alternatively, you could set an environment variable at the OS level."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"export OPENAI_API_BASE=http://localhost:8080/v1\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Then, when you make API calls from the ",(0,r.jsx)(n.code,{children:"client"}),", make sure that the ",(0,r.jsx)(n.code,{children:"model"})," is set to the model name\navailable on your node."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'response = client.chat.completions.create(\n    model="llama-3-8b-chat",\n    messages=[\n        {"role": "system", "content": "You are a strategic reasoner."},\n            {"role": "user", "content": "What is the purpose of life?"}\n        ],\n        temperature=0.7,\n        max_tokens=500\n    ]\n)\n'})}),"\n",(0,r.jsx)(n.p,{children:"That's it! You can now take any application built with the official OpenAI Python library and use your own\nLlamaEdge device as its backend!"})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var r=t(6540);const i={},s=r.createContext(i);function a(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);