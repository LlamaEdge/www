"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8220],{4531:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>d});var n=a(4848),o=a(8453);const s={sidebar_position:2},l="Quick start with LLM models",i={id:"user-guide/llm/get-started-with-llamaedge",title:"Quick start with LLM models",description:"LlamaEdge is a suite of component libraries and command line tools for developers to embed and run LLMs in their own apps. The best way to quickly experience LlamaEdge is to use easy-to-use utilities built on top of it.",source:"@site/docs/user-guide/llm/get-started-with-llamaedge.md",sourceDirName:"user-guide/llm",slug:"/user-guide/llm/get-started-with-llamaedge",permalink:"/docs/user-guide/llm/get-started-with-llamaedge",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/user-guide/llm/get-started-with-llamaedge.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"LLM",permalink:"/docs/category/llm"},next:{title:"Start an OpenAI compatible API server",permalink:"/docs/user-guide/llm/full-openai"}},r={},d=[{value:"Step 1: Install WasmEdge",id:"step-1-install-wasmedge",level:3},{value:"Step 2: Download the LLM model",id:"step-2-download-the-llm-model",level:3},{value:"Step 3: Download a portable API server app",id:"step-3-download-a-portable-api-server-app",level:3},{value:"Step 4: Chat with the chatbot UI",id:"step-4-chat-with-the-chatbot-ui",level:3}];function c(e){const t={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h3:"h3",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"quick-start-with-llm-models",children:"Quick start with LLM models"}),"\n",(0,n.jsx)(t.p,{children:"LlamaEdge is a suite of component libraries and command line tools for developers to embed and run LLMs in their own apps. The best way to quickly experience LlamaEdge is to use easy-to-use utilities built on top of it."}),"\n",(0,n.jsx)(t.p,{children:"It takes only a few minutes to start chatting with any open source LLM on your own laptop using LlamaEdge."}),"\n",(0,n.jsx)(t.h3,{id:"step-1-install-wasmedge",children:"Step 1: Install WasmEdge"}),"\n",(0,n.jsx)(t.p,{children:"First off, you'll need WasmEdge, a high-performance, lightweight, and extensible WebAssembly (Wasm) runtime optimized for server-side and edge computing. To install WasmEdge along with the necessary plugin for AI inference, open your terminal and execute the following command:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s\n"})}),"\n",(0,n.jsx)(t.p,{children:"This command fetches and runs the WasmEdge installation script, which automatically installs WasmEdge and the WASI-NN plugin, essential for running LLM models like Llama 3.1."}),"\n",(0,n.jsx)(t.h3,{id:"step-2-download-the-llm-model",children:"Step 2: Download the LLM model"}),"\n",(0,n.jsxs)(t.p,{children:["Next, you'll need to obtain a model file. For this tutorial, we're focusing on the ",(0,n.jsx)(t.strong,{children:"Llama 3.2 1B model finetuned for instruction following"}),", but the steps are generally applicable to other models too. Use the following command to download the model file."]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"curl -LO https://huggingface.co/second-state/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q5_K_M.gguf\n"})}),"\n",(0,n.jsx)(t.p,{children:"This command downloads the Llama-3.2-1B-Instruct model from Huggingface, an AI model hosting platform."}),"\n",(0,n.jsx)(t.h3,{id:"step-3-download-a-portable-api-server-app",children:"Step 3: Download a portable API server app"}),"\n",(0,n.jsxs)(t.p,{children:["Next, you need an application that can build an OpenAI compatible API server for the model.\nThe ",(0,n.jsx)(t.a,{href:"https://github.com/LlamaEdge/LlamaEdge/tree/main/llama-api-server",children:"LlamaEdge api server app"})," is a lightweight and cross-platform Wasm app that works on any device\nyou might have. Just download the compiled binary app."]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"curl -LO https://github.com/second-state/LlamaEdge/releases/latest/download/llama-api-server.wasm\n"})}),"\n",(0,n.jsxs)(t.blockquote,{children:["\n",(0,n.jsx)(t.p,{children:"The LlamaEdge apps are written in Rust and compiled to portable Wasm. That means they can run across devices and OSes without any change to the binary apps. You can simply download and run the compiled wasm apps regardless of your platform."}),"\n"]}),"\n",(0,n.jsx)(t.h3,{id:"step-4-chat-with-the-chatbot-ui",children:"Step 4: Chat with the chatbot UI"}),"\n",(0,n.jsxs)(t.p,{children:["The ",(0,n.jsx)(t.code,{children:"llama-api-server.wasm"})," is a web server with an OpenAI-compatible API. You still need HTML files for the chatbot UI.\nDownload and unzip the HTML UI files as follows."]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"curl -LO https://github.com/LlamaEdge/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz\ntar xzf chatbot-ui.tar.gz\nrm chatbot-ui.tar.gz\n"})}),"\n",(0,n.jsx)(t.p,{children:"Then, start the web server."}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"wasmedge --dir .:. --nn-preload default:GGML:AUTO:Llama-3.2-1B-Instruct-Q5_K_M.gguf llama-api-server.wasm -p llama-3-chat\n"})}),"\n",(0,n.jsxs)(t.p,{children:["Go to ",(0,n.jsx)(t.code,{children:"http://localhost:8080"})," on your computer to access the chatbot UI on a web page!"]}),"\n",(0,n.jsxs)(t.p,{children:["Congratulations! You have now started an LLM app on your own device. But if you are interested in running an agentic app beyond the simple chatbot, you will need to start an API server for this LLM along with the embedding model. Check out ",(0,n.jsx)(t.a,{href:"/docs/user-guide/openai-api/intro",children:"this guide on how to do it"}),"!"]})]})}function h(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},8453:(e,t,a)=>{a.d(t,{R:()=>l,x:()=>i});var n=a(6540);const o={},s=n.createContext(o);function l(e){const t=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:l(e.components),n.createElement(s.Provider,{value:t},e.children)}}}]);