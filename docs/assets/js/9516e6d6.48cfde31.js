"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2659],{4006:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var a=t(4848),i=t(8453);const s={sidebar_position:1},r="Start the llama-nexus API service",o={id:"llama-nexus/openai-api/intro",title:"Start the llama-nexus API service",description:"Since LlamaEdge provides an OpenAI-compatible API service, it can be a drop-in replacement for OpenAI in almost all LLM applications and frameworks.",source:"@site/docs/llama-nexus/openai-api/intro.md",sourceDirName:"llama-nexus/openai-api",slug:"/llama-nexus/openai-api/intro",permalink:"/docs/llama-nexus/openai-api/intro",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/llama-nexus/openai-api/intro.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Ecosystem apps",permalink:"/docs/category/ecosystem-apps"},next:{title:"LobeChat",permalink:"/docs/llama-nexus/openai-api/lobechat"}},l={},c=[{value:"OpenAI replacement",id:"openai-replacement",level:2},{value:"The OpenAI Python library",id:"the-openai-python-library",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"start-the-llama-nexus-api-service",children:"Start the llama-nexus API service"}),"\n",(0,a.jsx)(n.p,{children:"Since LlamaEdge provides an OpenAI-compatible API service, it can be a drop-in replacement for OpenAI in almost all LLM applications and frameworks.\nYou can start LlamaEdge API servers for individual AI models, and use llama-nexus to combine multiple AI models\ninto a single API server."}),"\n",(0,a.jsx)(n.h2,{id:"openai-replacement",children:"OpenAI replacement"}),"\n",(0,a.jsx)(n.p,{children:"In general, for any OpenAI tool, you could just replace the following."}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Config option"}),(0,a.jsx)(n.th,{children:"Value"}),(0,a.jsx)(n.th,{children:"Note"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"API endpoint URL"}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"http://localhost:8080/v1"})}),(0,a.jsx)(n.td,{children:"If the server is accessible from the web, you could use the public IP and port"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Model Name (for LLM)"}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"llama-3-8b-chat"})}),(0,a.jsxs)(n.td,{children:["The first value specified in the ",(0,a.jsx)(n.code,{children:"--model-name"})," option"]})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Model Name (for Text embedding)"}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"nomic-embed"})}),(0,a.jsxs)(n.td,{children:["The second value specified in the ",(0,a.jsx)(n.code,{children:"--model-name"})," option"]})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"API key"}),(0,a.jsx)(n.td,{children:"Empty"}),(0,a.jsx)(n.td,{children:"Or any value if the app does not permit empty string"})]})]})]}),"\n",(0,a.jsx)(n.h2,{id:"the-openai-python-library",children:"The OpenAI Python library"}),"\n",(0,a.jsxs)(n.p,{children:["You can install the ",(0,a.jsx)(n.a,{href:"https://pypi.org/project/openai/",children:"official OpenAI Python library"})," as follows."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"pip install openai\n"})}),"\n",(0,a.jsxs)(n.p,{children:["When you create an OpenAI client using the library, you can pass in the API endpoint point as the ",(0,a.jsx)(n.code,{children:"base_url"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'import openai\n\nclient = openai.OpenAI(base_url="http://localhost:8080/v1", api_key="")\n'})}),"\n",(0,a.jsx)(n.p,{children:"Alternatively, you could set an environment variable at the OS level."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"export OPENAI_API_BASE=http://localhost:8080/v1\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Then, when you make API calls from the ",(0,a.jsx)(n.code,{children:"client"}),", make sure that the ",(0,a.jsx)(n.code,{children:"model"})," is set to the model name\navailable on your node."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'response = client.chat.completions.create(\n    model="llama-3-8b-chat",\n    messages=[\n        {"role": "system", "content": "You are a strategic reasoner."},\n            {"role": "user", "content": "What is the purpose of life?"}\n        ],\n        temperature=0.7,\n        max_tokens=500\n    ]\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:"That's it! You can now take any application built with the official OpenAI Python library and use your own\nLlamaEdge device as its backend!"})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var a=t(6540);const i={},s=a.createContext(i);function r(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);