"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[83],{547:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>r,contentTitle:()=>i,default:()=>c,frontMatter:()=>l,metadata:()=>o,toc:()=>p});var s=t(4848),n=t(8453);const l={sidebar_position:3},i="LlamaEdge vs llama.cpp",o={id:"llamaedge_vs_llamacpp",title:"LlamaEdge vs llama.cpp",description:"LlamaEdge is built upon llama.cpp. So, why do you need the wrapper? Can you just write applications that directly",source:"@site/docs/llamaedge_vs_llamacpp.md",sourceDirName:".",slug:"/llamaedge_vs_llamacpp",permalink:"/docs/llamaedge_vs_llamacpp",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/llamaedge_vs_llamacpp.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"LlamaEdge vs Python",permalink:"/docs/llamaedge_vs_python"},next:{title:"LlamaEdge vs Ollama",permalink:"/docs/llamaedge_vs_ollama"}},r={},p=[];function d(e){const a={a:"a",h1:"h1",li:"li",p:"p",ul:"ul",...(0,n.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a.h1,{id:"llamaedge-vs-llamacpp",children:"LlamaEdge vs llama.cpp"}),"\n",(0,s.jsx)(a.p,{children:"LlamaEdge is built upon llama.cpp. So, why do you need the wrapper? Can you just write applications that directly\nlinks and compiles against the llama.cpp's C++ API? Yes, you sure can. But ..."}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:"LlamaEdge apps are written in Rust and compiled to portable Wasm. There is no need to incorporate and link against llama.cpp native library in your own app. You do not need a complex and fragile build system to manage cross-platform C++."}),"\n",(0,s.jsx)(a.li,{children:"LlamaEdge installer auto-detects the user's platform and installs the correct drivers and libraries. There is no need for you to create 100+ platform-dependent compile targets and then a complex installer to figure out where to install them."}),"\n",(0,s.jsx)(a.li,{children:"LlamaEdge provides high-level Rust components to handle application data on top of llama.cpp. Examples include prompt template management, context (RAG) management for prompts, API servers etc."}),"\n"]}),"\n",(0,s.jsxs)(a.p,{children:["Finally, LlamaEdge is designed to be agnostic to the underlying native runtimes. You can swap out llama.cpp for a different LLM\nruntime, such as ",(0,s.jsx)(a.a,{href:"https://github.com/WasmEdge/WasmEdge/issues/3260",children:"Intel neural speed engine"})," and ",(0,s.jsx)(a.a,{href:"https://github.com/WasmEdge/WasmEdge/issues/3266",children:"Apple MLX runtime"}),", without changing or even recompiling the application code."]}),"\n",(0,s.jsxs)(a.p,{children:["Besides LLMs, LlamaEdge is equipped by support runtimes for other types of AI models, such as\n",(0,s.jsx)(a.a,{href:"https://github.com/WasmEdge/WasmEdge/issues/3405",children:"stable diffusion"}),", ",(0,s.jsx)(a.a,{href:"https://github.com/WasmEdge/WasmEdge/issues/2768",children:"Yolo"}),", ",(0,s.jsx)(a.a,{href:"https://github.com/WasmEdge/WasmEdge/issues/3287",children:"whisper.cpp"}),", and ",(0,s.jsx)(a.a,{href:"https://github.com/WasmEdge/mediapipe-rs",children:"Google MediaPipe"}),"."]})]})}function c(e={}){const{wrapper:a}={...(0,n.R)(),...e.components};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,a,t)=>{t.d(a,{R:()=>i,x:()=>o});var s=t(6540);const n={},l=s.createContext(n);function i(e){const a=s.useContext(l);return s.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:i(e.components),s.createElement(l.Provider,{value:a},e.children)}}}]);