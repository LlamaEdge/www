"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[83],{547:(e,a,s)=>{s.r(a),s.d(a,{assets:()=>c,contentTitle:()=>l,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>d});var t=s(4848),n=s(8453);const o={sidebar_position:3},l="LlamaEdge vs llama.cpp",i={id:"llamaedge_vs_llamacpp",title:"LlamaEdge vs llama.cpp",description:"The llama.cpp project is one of the inference backends for LlamaEdge. LlamaEdge provides high level application",source:"@site/docs/llamaedge_vs_llamacpp.md",sourceDirName:".",slug:"/llamaedge_vs_llamacpp",permalink:"/docs/llamaedge_vs_llamacpp",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/llamaedge_vs_llamacpp.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"LlamaEdge vs Python",permalink:"/docs/llamaedge_vs_python"},next:{title:"LlamaEdge vs Ollama",permalink:"/docs/llamaedge_vs_ollama"}},c={},d=[];function r(e){const a={a:"a",h1:"h1",p:"p",...(0,n.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(a.h1,{id:"llamaedge-vs-llamacpp",children:"LlamaEdge vs llama.cpp"}),"\n",(0,t.jsx)(a.p,{children:"The llama.cpp project is one of the inference backends for LlamaEdge. LlamaEdge provides high level application\ncomponents to interact with AI models, such as encoding and decoding data,\nmanaging prompts and contexts, knowledge supplement, and tool use. It simplifies how business applications could\nmake use of the models. LlamaEdge and llama.cpp are complementary technologies."}),"\n",(0,t.jsxs)(a.p,{children:["In fact, LlamaEdge is designed to be agnostic to the underlying native runtimes.\nYou can swap out llama.cpp for a different LLM\nruntime, such as ",(0,t.jsx)(a.a,{href:"https://github.com/WasmEdge/WasmEdge/issues/3260",children:"Intel neural speed engine"})," and ",(0,t.jsx)(a.a,{href:"https://github.com/WasmEdge/WasmEdge/issues/3266",children:"Apple MLX runtime"}),", without changing or even recompiling the application code."]}),"\n",(0,t.jsxs)(a.p,{children:["Besides LLMs, LlamaEdge could support runtimes for other types of AI models, such as\n",(0,t.jsx)(a.a,{href:"https://github.com/WasmEdge/WasmEdge/issues/3405",children:"stable diffusion"}),", ",(0,t.jsx)(a.a,{href:"https://github.com/WasmEdge/WasmEdge/issues/2768",children:"Yolo"}),", ",(0,t.jsx)(a.a,{href:"https://github.com/WasmEdge/WasmEdge/issues/3287",children:"whisper.cpp"}),", and ",(0,t.jsx)(a.a,{href:"https://github.com/WasmEdge/mediapipe-rs",children:"Google MediaPipe"}),"."]})]})}function p(e={}){const{wrapper:a}={...(0,n.R)(),...e.components};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(r,{...e})}):r(e)}},8453:(e,a,s)=>{s.d(a,{R:()=>l,x:()=>i});var t=s(6540);const n={},o=t.createContext(n);function l(e){const a=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function i(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:l(e.components),t.createElement(o.Provider,{value:a},e.children)}}}]);